---
title: "Docker Compose Avanc√©"
description: "Healthchecks, profiles, scaling et bonnes pratiques Compose"
order: 9
duration: "30 min"
icon: "‚ö°"
xpReward: 250
objectives:
  - "Configurer des healthchecks et d√©pendances"
  - "Utiliser les profiles et overrides"
  - "Ma√Ætriser le scaling et les extensions YAML"
---

# Docker Compose Avanc√©

Maintenant qu'on ma√Ætrise les bases de Docker Compose, on va explorer des fonctionnalit√©s plus pouss√©es qui te permettront de cr√©er des applications vraiment robustes et professionnelles.

## 9.1 Healthchecks dans Compose

### Le probl√®me avec les conteneurs "d√©marr√©s"

Imagine : ton conteneur PostgreSQL d√©marre, Docker dit "OK c'est bon !", mais en r√©alit√© la base de donn√©es met encore 5 secondes √† √™tre vraiment pr√™te √† accepter des connexions. Si ton application web se lance en m√™me temps, elle va planter en essayant de se connecter.

C'est l√† qu'interviennent les **healthchecks** : ils permettent √† Docker de v√©rifier que ton service n'est pas juste d√©marr√©, mais vraiment **fonctionnel**.

### Anatomie d'un healthcheck

Un healthcheck est compos√© de plusieurs param√®tres :

- **test** : la commande qui v√©rifie que tout va bien
- **interval** : toutes les combien on lance le test (ex: toutes les 30s)
- **timeout** : temps max pour que le test r√©ponde
- **retries** : nombre d'√©checs avant de marquer le service comme "unhealthy"
- **start_period** : temps de gr√¢ce au d√©marrage (le test peut √©chouer sans cons√©quence)

### Exemples concrets

#### Healthcheck pour une API web

```yaml
services:
  web:
    image: nginx:alpine
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

Si tu n'as pas `curl` dans ton image, tu peux utiliser `wget` :

```yaml
test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/health"]
```

Ou m√™me un simple test TCP :

```yaml
test: ["CMD-SHELL", "nc -z localhost 80 || exit 1"]
```

#### Healthcheck pour PostgreSQL

PostgreSQL vient avec un outil d√©di√© : `pg_isready`

```yaml
services:
  database:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: secret123
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
```

#### Healthcheck pour MySQL

MySQL utilise `mysqladmin` :

```yaml
services:
  mysql:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: secret123
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p$$MYSQL_ROOT_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 3
```

#### Healthcheck pour Redis

```yaml
services:
  cache:
    image: redis:alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
```

### Voir l'√©tat de sant√©

Une fois lanc√©, tu peux v√©rifier l'√©tat :

```bash
docker compose ps
```

Tu verras dans la colonne "Status" :

- `(healthy)` : tout va bien !
- `(health: starting)` : en cours de v√©rification
- `(unhealthy)` : le service a rat√© trop de tests

Tu peux aussi inspecter les d√©tails :

```bash
docker inspect --format='{{json .State.Health}}' nom_du_conteneur | jq
```

## 9.2 depends_on avec condition: service_healthy

### Le probl√®me avec depends_on basique

Par d√©faut, `depends_on` dit juste √† Docker : "attends que le conteneur de la base d√©marre avant de lancer l'app". Mais **d√©marrer ‚â† √™tre pr√™t** !

```yaml
# ‚ùå Probl√©matique : l'app va probablement crasher
services:
  web:
    build: .
    depends_on:
      - database  # attend juste que le conteneur d√©marre

  database:
    image: postgres:15
```

### La solution : condition: service_healthy

Combin√© avec un healthcheck, tu peux dire : "attends que la base soit **vraiment pr√™te**" !

```yaml
services:
  web:
    build: .
    depends_on:
      database:
        condition: service_healthy  # attend que le healthcheck passe ‚úÖ

  database:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: secret123
      POSTGRES_DB: myapp
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 3s
      retries: 5
```

### Exemple complet : app 3-tiers

Voici une vraie app avec une file de messages, une API et une base :

```yaml
version: '3.8'

services:
  # Base de donn√©es
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: secret123
      POSTGRES_DB: myapp
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 3s
      retries: 5

  # Cache Redis
  redis:
    image: redis:alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # API Backend - attend que postgres ET redis soient pr√™ts
  api:
    build: ./api
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Frontend web - attend que l'API soit pr√™te
  web:
    build: ./web
    ports:
      - "80:80"
    depends_on:
      api:
        condition: service_healthy
```

Avec cette config, l'ordre de d√©marrage est **parfait** :

1. PostgreSQL et Redis d√©marrent en parall√®le
2. Ils passent leurs healthchecks
3. L'API d√©marre et attend d'√™tre healthy
4. Le frontend d√©marre en dernier

Z√©ro crash, z√©ro race condition ! üéØ

### Autres conditions disponibles

```yaml
depends_on:
  database:
    condition: service_started    # juste d√©marr√© (par d√©faut)
    condition: service_healthy    # healthcheck OK
    condition: service_completed_successfully  # pour les conteneurs qui s'arr√™tent apr√®s une t√¢che
```

## 9.3 Profiles dev vs prod

### Le concept des profiles

Les **profiles** te permettent d'activer certains services uniquement dans certains contextes. Genre, tu veux un outil de debug seulement en dev, ou un syst√®me de monitoring seulement en prod.

### Exemple basique

```yaml
services:
  # Service toujours actif
  web:
    image: nginx
    ports:
      - "80:80"

  # Service actif uniquement avec --profile dev
  adminer:
    image: adminer
    profiles: ["dev"]
    ports:
      - "8080:8080"

  # Service actif uniquement avec --profile prod
  prometheus:
    image: prom/prometheus
    profiles: ["prod"]
    ports:
      - "9090:9090"
```

### Utilisation

```bash
# Environnement dev : web + adminer
docker compose --profile dev up

# Environnement prod : web + prometheus
docker compose --profile prod up

# Tout activer (d√©conseill√© mais possible)
docker compose --profile dev --profile prod up

# Sans profile : seulement web
docker compose up
```

### Cas d'usage r√©el : d√©veloppement vs production

```yaml
version: '3.8'

services:
  # Core app - toujours actif
  api:
    build: ./api
    environment:
      NODE_ENV: ${NODE_ENV:-production}
    ports:
      - "3000:3000"

  database:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: secret123

  # Outils de d√©veloppement
  pgadmin:
    image: dpage/pgadmin4
    profiles: ["dev"]
    environment:
      PGADMIN_DEFAULT_EMAIL: dev@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"

  mailhog:  # Intercepte les emails en dev
    image: mailhog/mailhog
    profiles: ["dev"]
    ports:
      - "8025:8025"  # Interface web

  # Services de debug avanc√©s
  debug-tools:
    image: nicolaka/netshoot
    profiles: ["debug"]
    command: sleep infinity
    network_mode: "host"

  # Monitoring production
  grafana:
    image: grafana/grafana
    profiles: ["prod", "monitoring"]
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana

  prometheus:
    image: prom/prometheus
    profiles: ["prod", "monitoring"]
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus

volumes:
  grafana-data:
  prometheus-data:
```

### Profiles multiples

Un service peut avoir plusieurs profiles :

```yaml
services:
  monitoring:
    image: monitoring-tool
    profiles: ["dev", "prod"]  # actif dans les deux contextes
```

### Astuce : variable d'environnement

Tu peux d√©finir le profile par d√©faut :

```bash
# Dans ton .env
COMPOSE_PROFILES=dev

# Maintenant, docker compose up active automatiquement le profile dev
docker compose up
```

## 9.4 Override files

### Le syst√®me de fichiers multiples

Docker Compose peut **fusionner** plusieurs fichiers YAML. √áa permet de s√©parer :

- La config de base (commune)
- Les sp√©cificit√©s de chaque environnement

### Fichier automatique : docker-compose.override.yml

Par d√©faut, si ce fichier existe, il est **automatiquement charg√©** :

```bash
# Ces deux commandes sont identiques :
docker compose up
docker compose -f docker-compose.yml -f docker-compose.override.yml up
```

#### Exemple : config de base

**docker-compose.yml** (commit√© dans Git)

```yaml
version: '3.8'

services:
  web:
    image: nginx:alpine
    volumes:
      - ./html:/usr/share/nginx/html

  database:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
    secrets:
      - db_password

secrets:
  db_password:
    file: ./secrets/db_password.txt
```

#### Override local (pas dans Git)

**docker-compose.override.yml** (dans .gitignore)

```yaml
version: '3.8'

services:
  web:
    ports:
      - "8080:80"  # expose le port seulement en local

  database:
    ports:
      - "5432:5432"  # acc√®s direct √† la DB en dev
    environment:
      POSTGRES_PASSWORD: dev_password  # override le secret en dev
```

Le r√©sultat fusionn√© sera :

```yaml
services:
  web:
    image: nginx:alpine
    volumes:
      - ./html:/usr/share/nginx/html
    ports:
      - "8080:80"  # ajout√© par override

  database:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: dev_password  # remplac√© par override
    ports:
      - "5432:5432"  # ajout√© par override
```

### Fichiers multiples explicites

Pour la prod, tu peux utiliser des fichiers sp√©cifiques :

**docker-compose.prod.yml**

```yaml
version: '3.8'

services:
  web:
    image: myregistry.com/web:v1.2.3  # image build√©e
    restart: always
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  database:
    volumes:
      - db-data:/var/lib/postgresql/data  # volume persistant
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

volumes:
  db-data:
    driver: local
```

Utilisation :

```bash
# Dev (avec override auto)
docker compose up

# Prod (fichier explicite)
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Test (plusieurs fichiers)
docker compose -f docker-compose.yml -f docker-compose.test.yml run tests
```

### Pattern avanc√© : base + environnement

Structure de projet :

```
project/
‚îú‚îÄ‚îÄ docker-compose.yml           # base commune
‚îú‚îÄ‚îÄ docker-compose.override.yml  # dev local (gitignored)
‚îú‚îÄ‚îÄ docker-compose.dev.yml       # dev partag√©
‚îú‚îÄ‚îÄ docker-compose.prod.yml      # production
‚îú‚îÄ‚îÄ docker-compose.test.yml      # CI/CD
‚îî‚îÄ‚îÄ .env.example
```

**docker-compose.yml** (base)

```yaml
services:
  api:
    build:
      context: ./api
      target: ${BUILD_TARGET:-production}
    environment:
      DATABASE_URL: postgresql://postgres:${DB_PASSWORD}@database:5432/myapp

  database:
    image: postgres:15-alpine
```

**docker-compose.dev.yml**

```yaml
services:
  api:
    build:
      target: development
    volumes:
      - ./api:/app  # hot reload
    command: npm run dev
    environment:
      NODE_ENV: development
      DEBUG: "app:*"
```

**docker-compose.prod.yml**

```yaml
services:
  api:
    image: myregistry.com/api:${VERSION}
    restart: always
    environment:
      NODE_ENV: production
    deploy:
      replicas: 3
```

### R√®gles de fusion

- **Listes** (ports, volumes) : **ajout√©es** (cumul√©es)
- **Dictionnaires** (environment) : **merg√©s** (cl√©s overrid√©es)
- **Valeurs simples** : **remplac√©es**

```yaml
# Base
environment:
  KEY1: value1
  KEY2: value2

# Override
environment:
  KEY2: new_value2  # remplace
  KEY3: value3      # ajoute

# R√©sultat
environment:
  KEY1: value1
  KEY2: new_value2  # ‚úÖ remplac√©
  KEY3: value3      # ‚úÖ ajout√©
```

## 9.5 Resource limits

### Pourquoi limiter les ressources ?

Sans limites, un conteneur peut **bouffer tout le CPU et la RAM** de ta machine. Imagine un bug qui cr√©e une boucle infinie : boom, ton ordi freeze. üíÄ

### Syntaxe de base

```yaml
services:
  api:
    image: myapi
    deploy:
      resources:
        limits:       # Maximum autoris√©
          cpus: '0.5'      # 50% d'un CPU
          memory: 512M     # 512 MB max
        reservations: # Minimum garanti
          cpus: '0.25'     # 25% garanti
          memory: 256M     # 256 MB garantis
```

### Diff√©rence limits vs reservations

- **limits** : plafond absolu (le conteneur ne peut pas d√©passer)
- **reservations** : ressources r√©serv√©es (garanties m√™me sous forte charge)

### Exemples par type d'app

#### Application web l√©g√®re

```yaml
services:
  nginx:
    image: nginx:alpine
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
```

#### API Node.js moyenne

```yaml
services:
  api:
    build: ./api
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
```

#### Base de donn√©es PostgreSQL

```yaml
services:
  postgres:
    image: postgres:15
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    environment:
      # Configure aussi Postgres pour matcher les limites
      POSTGRES_SHARED_BUFFERS: 1GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 3GB
```

#### Worker de traitement intensif

```yaml
services:
  video-encoder:
    build: ./worker
    deploy:
      resources:
        limits:
          cpus: '4'      # peut utiliser 4 CPUs
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
```

### Autres options avanc√©es

```yaml
services:
  app:
    image: myapp
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
          pids: 100  # limite le nombre de processus
        reservations:
          cpus: '0.5'
          memory: 512M
          devices:
            - driver: nvidia
              count: 1  # r√©serve 1 GPU
              capabilities: [gpu]
```

### Monitoring des ressources

Surveille la consommation r√©elle :

```bash
# Voir l'usage en temps r√©el
docker stats

# Avec Compose
docker compose stats
```

Tu verras :

```
CONTAINER     CPU %     MEM USAGE / LIMIT     MEM %
myapp_api     45.32%    612MB / 1GB          61.2%
myapp_db      12.45%    1.8GB / 4GB          45.0%
```

### Exemple complet : stack optimis√©e

```yaml
version: '3.8'

services:
  # Reverse proxy - l√©ger
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # API - charge moyenne
  api:
    build: ./api
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Database - ressources importantes
  postgres:
    image: postgres:15-alpine
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  # Cache - l√©ger mais rapide
  redis:
    image: redis:alpine
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M

  # Worker - peut prendre du CPU
  worker:
    build: ./worker
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
```

### Attention en dev vs prod

En dev, les limites strictes peuvent √™tre p√©nibles. Utilise un override :

**docker-compose.override.yml** (dev)

```yaml
services:
  api:
    deploy:
      resources:
        limits:
          cpus: '4'      # plus g√©n√©reux en dev
          memory: 4G
```

## 9.6 Scaling services

### Le concept du scaling

**Scaler** = lancer plusieurs instances du m√™me service en parall√®le. Super utile pour g√©rer plus de charge !

### Scaling basique

```bash
# Lance 3 instances du service "api"
docker compose up -d --scale api=3

# Voir les conteneurs
docker compose ps
```

R√©sultat :

```
NAME              COMMAND       STATUS    PORTS
myapp-api-1       "npm start"   Up        3000/tcp
myapp-api-2       "npm start"   Up        3000/tcp
myapp-api-3       "npm start"   Up        3000/tcp
```

### Probl√®me : conflit de ports

Si tu as d√©fini un port dans ton docker-compose.yml, √ßa va planter :

```yaml
# ‚ùå Impossible de scaler
services:
  api:
    image: myapi
    ports:
      - "3000:3000"  # Conflit : 3 conteneurs veulent le m√™me port !
```

### Solution 1 : pas de ports expos√©s

Si tu as un reverse proxy (nginx), tes services n'ont pas besoin d'exposer de ports :

```yaml
services:
  # Reverse proxy - un seul
  nginx:
    image: nginx
    ports:
      - "80:80"
    depends_on:
      - api

  # API - scalable ‚úÖ
  api:
    image: myapi
    # Pas de ports ! nginx communique via le r√©seau interne
```

### Solution 2 : port dynamique

Utilise un port al√©atoire c√¥t√© host :

```yaml
services:
  api:
    image: myapi
    ports:
      - "3000"  # pas de "host:container", juste le port container
```

Docker va attribuer automatiquement des ports libres sur l'h√¥te :

```
myapp-api-1  ‚Üí  0.0.0.0:32771:3000
myapp-api-2  ‚Üí  0.0.0.0:32772:3000
myapp-api-3  ‚Üí  0.0.0.0:32773:3000
```

### Solution 3 : load balancer externe

La vraie solution pro : un load balancer qui distribue les requ√™tes.

```yaml
version: '3.8'

services:
  # Load balancer Nginx
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api

  # API - scalable
  api:
    build: ./api
    expose:
      - "3000"
    # Pas de ports publics
```

**nginx.conf** (load balancing)

```nginx
events {
    worker_connections 1024;
}

http {
    # Pool de backends
    upstream api_backend {
        server api:3000;  # Docker Compose r√©sout automatiquement vers TOUTES les instances
    }

    server {
        listen 80;

        location / {
            proxy_pass http://api_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
    }
}
```

Maintenant, tu peux scaler :

```bash
docker compose up -d --scale api=5
```

Nginx va distribuer les requ√™tes entre les 5 instances automatiquement ! üéØ

### D√©finir le scaling dans le fichier

```yaml
services:
  api:
    image: myapi
    deploy:
      replicas: 3  # d√©marre toujours 3 instances
```

Avec cette config, un simple `docker compose up` lance 3 instances.

### Exemple complet : app scalable

```yaml
version: '3.8'

services:
  # Reverse proxy
  traefik:
    image: traefik:v2.10
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--entrypoints.web.address=:80"
    ports:
      - "80:80"
      - "8080:8080"  # Dashboard Traefik
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  # API - auto-d√©couverte par Traefik
  api:
    build: ./api
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.api.rule=Host(`api.localhost`)"
      - "traefik.http.services.api.loadbalancer.server.port=3000"
    deploy:
      replicas: 3  # 3 instances d√®s le d√©part
    depends_on:
      - redis
      - postgres

  # Worker - scalable aussi
  worker:
    build: ./worker
    deploy:
      replicas: 2
    depends_on:
      - redis

  # Database - ne scale PAS (stateful)
  postgres:
    image: postgres:15
    volumes:
      - db-data:/var/lib/postgresql/data

  # Redis - peut scaler avec Redis Cluster (avanc√©)
  redis:
    image: redis:alpine

volumes:
  db-data:
```

### Scaling dynamique

Augmente ou diminue √† chaud :

```bash
# Passe de 3 √† 10 instances
docker compose up -d --scale api=10

# R√©duit √† 2 instances
docker compose up -d --scale api=2

# Retour au nombre d√©fini dans le fichier
docker compose up -d --scale api=3
```

### Limites du scaling avec Compose

‚ö†Ô∏è **Attention** : Docker Compose n'est pas un orchestrateur avanc√©. Pour du vrai scaling en production, utilise :

- **Docker Swarm** : scaling natif Docker
- **Kubernetes** : l'orchestrateur pro
- **Services manag√©s** : AWS ECS, Google Cloud Run, Azure Container Apps

Mais pour du dev local ou de petites prod, Compose + un bon load balancer fait largement le job !

## 9.7 Extensions YAML

Les **extensions YAML** te permettent de rendre ton docker-compose.yml **DRY** (Don't Repeat Yourself) en d√©finissant des blocs r√©utilisables.

### Syntaxe : cl√©s avec x-

Toute cl√© commen√ßant par `x-` est **ignor√©e par Docker Compose**, mais tu peux la r√©f√©rencer ailleurs.

```yaml
# D√©finition d'un bloc r√©utilisable
x-common-config: &common
  restart: always
  logging:
    driver: json-file
    options:
      max-size: "10m"
      max-file: "3"

services:
  api:
    image: myapi
    <<: *common  # injecte tout le contenu de x-common-config

  worker:
    image: myworker
    <<: *common  # m√™me config ici
```

C'est √©quivalent √† :

```yaml
services:
  api:
    image: myapi
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  worker:
    image: myworker
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
```

### Anchors (&) et Aliases (*)

C'est du **YAML pur** (pas sp√©cifique √† Docker) :

- `&nom` cr√©e une **ancre** (marque-page)
- `*nom` cr√©e un **alias** (copie le contenu de l'ancre)
- `<<: *nom` **merge** le contenu dans le parent

### Exemple : environnements communs

```yaml
x-app-env: &app-env
  DATABASE_URL: postgresql://postgres:secret@database:5432/myapp
  REDIS_URL: redis://cache:6379
  LOG_LEVEL: info

services:
  api:
    image: myapi
    environment:
      <<: *app-env
      SERVICE_NAME: api

  worker:
    image: myworker
    environment:
      <<: *app-env
      SERVICE_NAME: worker
      QUEUE_NAME: jobs
```

R√©sultat :

```yaml
services:
  api:
    environment:
      DATABASE_URL: postgresql://postgres:secret@database:5432/myapp
      REDIS_URL: redis://cache:6379
      LOG_LEVEL: info
      SERVICE_NAME: api

  worker:
    environment:
      DATABASE_URL: postgresql://postgres:secret@database:5432/myapp
      REDIS_URL: redis://cache:6379
      LOG_LEVEL: info
      SERVICE_NAME: worker
      QUEUE_NAME: jobs
```

### Exemple : healthchecks standardis√©s

```yaml
x-healthcheck-defaults: &healthcheck-defaults
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

services:
  api:
    image: myapi
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      <<: *healthcheck-defaults

  web:
    image: nginx
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      <<: *healthcheck-defaults
```

### Exemple : limites de ressources r√©utilisables

```yaml
x-resources-small: &resources-small
  limits:
    cpus: '0.5'
    memory: 512M
  reservations:
    cpus: '0.25'
    memory: 256M

x-resources-medium: &resources-medium
  limits:
    cpus: '1'
    memory: 1G
  reservations:
    cpus: '0.5'
    memory: 512M

services:
  nginx:
    image: nginx
    deploy:
      resources:
        <<: *resources-small

  api:
    image: myapi
    deploy:
      resources:
        <<: *resources-medium
      replicas: 3
```

### Exemple : logging commun

```yaml
x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"

services:
  api:
    image: myapi
    logging: *default-logging

  worker:
    image: myworker
    logging: *default-logging

  database:
    image: postgres
    logging: *default-logging
```

### Merger plusieurs ancres

Tu peux combiner plusieurs blocs :

```yaml
x-common: &common
  restart: always
  networks:
    - app-network

x-logging: &logging
  logging:
    driver: json-file
    options:
      max-size: "10m"

services:
  api:
    image: myapi
    <<: [*common, *logging]  # merge les deux !
```

### Extension compl√®te : template de service

```yaml
x-service-template: &service-template
  restart: always
  networks:
    - app-network
  logging:
    driver: json-file
    options:
      max-size: "10m"
      max-file: "3"
  deploy:
    resources:
      limits:
        cpus: '1'
        memory: 1G
  environment: &common-env
    LOG_LEVEL: ${LOG_LEVEL:-info}
    NODE_ENV: ${NODE_ENV:-production}

services:
  api:
    <<: *service-template
    build: ./api
    environment:
      <<: *common-env
      SERVICE_NAME: api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s

  worker:
    <<: *service-template
    build: ./worker
    environment:
      <<: *common-env
      SERVICE_NAME: worker

networks:
  app-network:
```

### Astuce pro : fichier de fragments

Pour les tr√®s gros projets, tu peux m√™me s√©parer les extensions :

**docker-compose.fragments.yml**

```yaml
x-healthcheck-defaults: &healthcheck-defaults
  interval: 30s
  timeout: 10s
  retries: 3

x-resources-api: &resources-api
  limits:
    cpus: '1'
    memory: 1G
```

**docker-compose.yml**

```yaml
version: '3.8'

# Import des fragments (YAML 1.2)
<<: [*healthcheck-defaults, *resources-api]

services:
  api:
    image: myapi
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:3000/health"]
      <<: *healthcheck-defaults
    deploy:
      resources:
        <<: *resources-api
```

Puis :

```bash
docker compose -f docker-compose.fragments.yml -f docker-compose.yml up
```

## 9.8 Exercice pratique

On va mettre en pratique TOUT ce qu'on a vu ! Tu vas construire une application 3-tiers compl√®te avec toutes les best practices.

### Objectif

Cr√©er une stack compl√®te :

- **Nginx** (reverse proxy)
- **API Node.js** (backend)
- **PostgreSQL** (database)
- **Redis** (cache)

Avec :

‚úÖ Healthchecks partout
‚úÖ D√©pendances `service_healthy`
‚úÖ Profiles dev/prod
‚úÖ Extensions YAML pour DRY
‚úÖ Limites de ressources
‚úÖ Scaling de l'API

### Preparation

Cree la structure du projet :

```bash
mkdir -p mon-app/nginx mon-app/api && cd mon-app
```

```
mon-app/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ docker-compose.dev.yml
‚îú‚îÄ‚îÄ docker-compose.prod.yml
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ nginx/
‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf
‚îî‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ package.json
    ‚îî‚îÄ‚îÄ server.js
```

Cree chaque fichier dans l'ordre ci-dessous.

### 1. L'API Node.js

**api/server.js**

```javascript
const express = require('express');
const { Pool } = require('pg');
const redis = require('redis');

const app = express();
const PORT = process.env.PORT || 3000;

// Database connection
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
});

// Redis connection
const redisClient = redis.createClient({
  url: process.env.REDIS_URL,
});

redisClient.connect();

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    // Check database
    await pool.query('SELECT 1');

    // Check redis
    await redisClient.ping();

    res.json({ status: 'healthy', timestamp: new Date() });
  } catch (error) {
    res.status(503).json({ status: 'unhealthy', error: error.message });
  }
});

// Example endpoint
app.get('/api/visits', async (req, res) => {
  try {
    // Increment visits in Redis
    const visits = await redisClient.incr('visits');

    // Log to database
    await pool.query(
      'INSERT INTO visits (count, timestamp) VALUES ($1, $2)',
      [visits, new Date()]
    );

    res.json({ visits });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.listen(PORT, () => {
  console.log(`API listening on port ${PORT}`);
});
```

**api/Dockerfile**

```dockerfile
FROM node:18-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci --omit=dev

# Copy source
COPY . .

# Install curl for healthcheck
RUN apk add --no-cache curl

EXPOSE 3000

CMD ["node", "server.js"]
```

**api/package.json**

```json
{
  "name": "api",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.0",
    "redis": "^4.6.7"
  }
}
```

### 2. Configuration Nginx

**nginx/nginx.conf**

```nginx
events {
    worker_connections 1024;
}

http {
    # Load balancing pour l'API
    upstream api_backend {
        server api:3000;
    }

    server {
        listen 80;
        server_name localhost;

        # Health check
        location /health {
            access_log off;
            return 200 "OK\n";
            add_header Content-Type text/plain;
        }

        # Proxy vers l'API
        location /api/ {
            proxy_pass http://api_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

            # Timeouts
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
        }

        # Page d'accueil statique
        location / {
            return 200 "Welcome to Docker Learn App!\n";
            add_header Content-Type text/plain;
        }
    }
}
```

### 3. Docker Compose - Base

**docker-compose.yml**

```yaml
version: '3.8'

# ============================================
# EXTENSIONS YAML (DRY)
# ============================================

x-healthcheck-defaults: &healthcheck-defaults
  interval: 10s
  timeout: 5s
  retries: 5
  start_period: 30s

x-common-env: &common-env
  NODE_ENV: ${NODE_ENV:-production}
  DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD:-secret123}@database:5432/${POSTGRES_DB:-myapp}
  REDIS_URL: redis://cache:6379

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

x-restart-policy: &restart-policy
  restart: unless-stopped

# ============================================
# SERVICES
# ============================================

services:
  # Reverse Proxy
  nginx:
    image: nginx:alpine
    <<: *restart-policy
    ports:
      - "${NGINX_PORT:-80}:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      <<: *healthcheck-defaults
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
    networks:
      - frontend
      - backend

  # API Backend
  api:
    build: ./api
    <<: *restart-policy
    environment:
      <<: *common-env
      PORT: 3000
    depends_on:
      database:
        condition: service_healthy
      cache:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      <<: *healthcheck-defaults
    logging: *default-logging
    deploy:
      replicas: ${API_REPLICAS:-2}
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    networks:
      - backend

  # PostgreSQL Database
  database:
    image: postgres:15-alpine
    <<: *restart-policy
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-secret123}
      POSTGRES_DB: ${POSTGRES_DB:-myapp}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 5s
      timeout: 3s
      retries: 5
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - backend

  # Redis Cache
  cache:
    image: redis:alpine
    <<: *restart-policy
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    networks:
      - backend

# ============================================
# NETWORKS
# ============================================

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true  # pas d'acc√®s internet direct

# ============================================
# VOLUMES
# ============================================

volumes:
  postgres-data:
  redis-data:
```

### 4. Override Dev

**docker-compose.dev.yml**

```yaml
version: '3.8'

services:
  # Outils de debug seulement en dev
  pgadmin:
    image: dpage/pgadmin4
    profiles: ["dev"]
    environment:
      PGADMIN_DEFAULT_EMAIL: dev@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    networks:
      - backend

  redis-commander:
    image: rediscommander/redis-commander
    profiles: ["dev"]
    environment:
      REDIS_HOSTS: cache:cache:6379
    ports:
      - "8081:8081"
    networks:
      - backend

  # Override de l'API pour le dev
  api:
    build:
      context: ./api
      dockerfile: Dockerfile.dev
    volumes:
      - ./api:/app
      - /app/node_modules
    command: npm run dev  # hot reload
    environment:
      NODE_ENV: development
      DEBUG: "app:*"
    ports:
      - "3000:3000"  # expose directement en dev

  # Expose la DB en dev
  database:
    ports:
      - "5432:5432"

  # Expose Redis en dev
  cache:
    ports:
      - "6379:6379"
```

### 5. Override Prod

**docker-compose.prod.yml**

```yaml
version: '3.8'

services:
  # Monitoring seulement en prod
  prometheus:
    image: prom/prometheus
    profiles: ["prod", "monitoring"]
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - backend

  grafana:
    image: grafana/grafana
    profiles: ["prod", "monitoring"]
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3001:3000"
    networks:
      - backend

  # Config prod API
  api:
    image: myregistry.com/api:${VERSION:-latest}
    deploy:
      replicas: 5  # plus d'instances en prod
      resources:
        limits:
          cpus: '2'
          memory: 2G

  # Config prod Database
  database:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    # Backup automatique
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./backups:/backups

volumes:
  prometheus-data:
  grafana-data:
```

### 6. Fichier d'init SQL

**init.sql**

```sql
-- Create visits table
CREATE TABLE IF NOT EXISTS visits (
    id SERIAL PRIMARY KEY,
    count INTEGER NOT NULL,
    timestamp TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Create index
CREATE INDEX idx_visits_timestamp ON visits(timestamp);

-- Insert initial data
INSERT INTO visits (count, timestamp) VALUES (0, NOW());
```

### 7. Variables d'environnement

**.env.example**

```bash
# Environment
NODE_ENV=production

# Database
POSTGRES_USER=postgres
POSTGRES_PASSWORD=changeme_in_production
POSTGRES_DB=myapp

# Nginx
NGINX_PORT=80

# Scaling
API_REPLICAS=2
```

### 8. Utilisation

#### D√©veloppement

```bash
# Copier les variables
cp .env.example .env

# Lancer la stack dev avec outils
docker compose -f docker-compose.yml -f docker-compose.dev.yml --profile dev up -d

# Acc√®s :
# - App : http://localhost
# - API : http://localhost:3000
# - PgAdmin : http://localhost:5050
# - Redis Commander : http://localhost:8081
```

#### Production

```bash
# Lancer la stack prod avec monitoring
docker compose -f docker-compose.yml -f docker-compose.prod.yml --profile prod up -d

# Scaler l'API dynamiquement
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale api=10

# Acc√®s :
# - App : http://localhost
# - Prometheus : http://localhost:9090
# - Grafana : http://localhost:3001
```

#### Tests

```bash
# Tester le healthcheck de l'API
curl http://localhost/api/health

# Tester le compteur de visites
curl http://localhost/api/visits

# Voir les logs
docker compose logs -f api

# Voir les ressources
docker compose stats
```

### 9. V√©rifications

Checklist pour v√©rifier que tout fonctionne :

```bash
# 1. Tous les services sont healthy
docker compose ps
# Tous doivent √™tre "Up (healthy)"

# 2. L'ordre de d√©marrage est respect√©
docker compose logs | grep "started"
# database et cache d'abord, puis api, puis nginx

# 3. Le scaling fonctionne
docker compose up -d --scale api=3
docker compose ps
# Tu dois voir api-1, api-2, api-3

# 4. Nginx distribue les requ√™tes
for i in {1..10}; do curl -s http://localhost/api/visits; done
# Les visites augmentent, les requ√™tes sont distribu√©es

# 5. Les healthchecks fonctionnent
docker compose exec api curl -f http://localhost:3000/health
# {"status":"healthy",...}

# 6. Les limites de ressources sont actives
docker stats --no-stream
# V√©rifie que les limites sont appliqu√©es
```

### 10. Points cl√©s de l'exercice

Tu as maintenant une stack qui utilise :

‚úÖ **Healthchecks** sur tous les services
‚úÖ **depends_on** avec `service_healthy` pour un ordre de d√©marrage parfait
‚úÖ **Profiles** pour s√©parer dev/prod
‚úÖ **Override files** pour personnaliser chaque environnement
‚úÖ **Extensions YAML** (`x-`) pour √©viter les r√©p√©titions
‚úÖ **Resource limits** pour contr√¥ler CPU/RAM
‚úÖ **Scaling** de l'API avec load balancing automatique
‚úÖ **R√©seaux** s√©par√©s (frontend/backend)
‚úÖ **Volumes** pour persister les donn√©es

C'est une vraie architecture production-ready ! üöÄ

---

## R√©cap final

Tu as appris √† :

- **Healthchecks** : v√©rifier que tes services sont vraiment fonctionnels
- **depends_on avanc√©** : attendre que les d√©pendances soient healthy
- **Profiles** : activer des services selon l'environnement
- **Override files** : personnaliser sans modifier la base
- **Resource limits** : emp√™cher un service de tout bouffer
- **Scaling** : lancer plusieurs instances pour g√©rer la charge
- **Extensions YAML** : rendre tes fichiers DRY et maintenables

Avec ces techniques, tu peux cr√©er des applications Docker Compose **robustes, scalables et professionnelles** ! üí™

La prochaine √©tape ? D√©couvrir les orchestrateurs comme **Kubernetes** pour g√©rer des centaines de conteneurs en production. Mais tu as maintenant des bases **solides** ! üéØ
